#cloud-config
fqdn: ${fqdn}
hostname: ${name}
manage_etc_hosts: true
prefer_fqdn_over_hostname: true
preserve_hostname: false
repo_upgrade: all

packages:
  - nmap-ncat
  - jq
  - curl

bootcmd:
  - while [[ ! -b $(readlink -f /dev/nvme1n1) ]]; do echo "waiting for the disk..."; sleep 5; done
  - mkfs.xfs -L data /dev/nvme1n1
  - mkdir -p /data

runcmd:
  - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - yum -y install consul
  - systemctl enable consul
  - systemctl start consul
  - yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release enable mysql-shell
  - yum -y install percona-mysql-shell
  - percona-release enable tools release
  - yum install -y pmm2-client
  - bash /tmp/waiter.sh readyz
  - pmm-admin config --az="us-east-1f" --region="us-east-1" --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - percona-release enable-only pdps-8x-innovation
  - yum -y install percona-server-server
  - systemctl enable mysql
  - systemctl start mysql
  - bash /root/init-mysql.sh
  - sleep 10 && bash /tmp/waiter.sh mysql
  - pmm-admin add mysql --metrics-mode=push --username=pmm-admin --password='${mysql_root_password}' --cluster='percona-group-replication-81' --replication-set='percona-group-replication-81' --environment='"${environment_name}" --query-source=slowlog --service-name=${name}
  - wget --quiet https://github.com/ncabatoff/process-exporter/releases/download/v0.7.10/process-exporter_0.7.10_linux_386.rpm
  - yum -y install process-exporter_0.7.10_linux_386.rpm
  - service process-exporter start
  - bash /tmp/waiter.sh process-exporter
  - pmm-admin add external --group=processes --listen-port=9256 --environment="${environment_name}" --service-name="${name}-processes" --cluster="processes-cluster"


mounts:
  - ["/dev/nvme1n1", "/data", "xfs", "defaults,noatime", "0", "2"]

write_files:
  - path: /etc/resolv.conf
    permissions: "0644"
    content: |
      ; generated by #cloud-config
      search ${local_domain} ec2.internal
      options timeout:2 attempts:5
      nameserver 10.0.0.2
  - path: /root/consul-check-mysql.sh
    permissions: "0700"
    content: |
      #!/bin/bash
      # Set MySQL server credentials
      MYSQL_USER="consul-health-check"
      MYSQL_PASSWORD="${mysql_root_password}"
      MYSQL_SOCKET="/data/mysql.sock"

      # Attempt to connect using MySQL Shell with a 2-second timeout
      mysqlsh --socket="$MYSQL_SOCKET" --user="$MYSQL_USER" --password="$MYSQL_PASSWORD" --connect-timeout=2 << EOF

      SELECT 1;

      EOF

      # Check the exit status of mysqlsh to determine connection success
      if [[ $? -ne 0 ]]; then
        echo "Error: Failed to connect to MySQL server." >&2
        exit 1
      fi

      echo "Connection to MySQL server successful."
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "bastion"]
      server = false
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/percona-group-replication-service.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "percona-group-replication",
          "name": "percona-group-replication",
          "port": 3306,
          "tags": ["${name}"],
          "checks": [
            {
            "ID": "percona-group-replication_check_mysqladmin",
            "Interval": "3s",
            "Name": "Check for MySQL running on ${name}.",
            "Notes": "Check for MySQL running on ${name}. Monitoring of MySQL metrics are enabled with this service.",
            "ServiceID": "percona-group-replication_check_mysqladmin",
            "Success_before_passing": 3,
            "Timeout": "2s",
            "args": ["mysqladmin", "ping", "--host=127.0.0.1", "--port=3306", "--user='consul-health-check'", "--password=${mysql_root_password}"]
            }
          ]
        }
      }
  - path: /etc/consul.d/process-exporter-check-http.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "process-exporter",
          "name": "process-exporter",
          "port": 9256,
          "tags": ["${name}"],
          "checks": [
            {
            "HTTP": "http://${fqdn}:9256/metrics",
            "ID": "process-exporter_check_http",
            "Interval": "3s",
            "Method": "GET",
            "Name": "Check for process-exporter using HTTP",
            "Notes": "Check for process-exporter using HTTP. Monitoring of per-process metrics are enabled with this service.",
            "ServiceID": "process-exporter_check_http",
            "Success_before_passing": 3,
            "Timeout": "5s"
            }
          ]
        }
      }
  - path: /tmp/waiter.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Script to do all the waiting
      service="$1"
      if [[ $service == "process-exporter" ]]; then
        # process-exporter
        while true; do
          # Get the status of the process-exporter_check_http check
          status=$(dig @127.0.0.1 -p 8600 ${name}.process-exporter.service.consul SRV | awk '/SRV.*${fqdn}\.$/ {print $1}')

          if [[ $status == "${name}.process-exporter.service.consul." ]]; then
            echo "process-exporter check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "process-exporter check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "proxysql" ]]; then
        # proxysql
        while true; do
          # Get the status of the Proxysql check
          status=$(dig @127.0.0.1 -p 8600 proxysql.service.consul SRV | awk '/SRV.*proxysql.${environment_name}.local.$/ {print $1}')

          if [[ $status == "proxysql.service.consul." ]]; then
            echo "ProxySQL check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "ProxySQL check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "readyz" ]] ; then
        # PMM readyz
        while true; do
          # Check for DNS :facepalm:
          dnsstatus=$(dig pmm-server.${environment_name}.local A | awk '/A.*10.*$/ {print $1}')

          if [[ $dnsstatus == "pmm-server.${environment_name}.local." ]]; then
            echo "PMMreadyz_check_http DNS check is passing."

            # Get the status of the PMMreadyz_check_http check
            status=$(dig @127.0.0.1 -p 8600 pmmreadyz.service.consul SRV | awk '/SRV.*bastion.${environment_name}.local.$/ {print $1}')
        
            if [[ $status == "pmmreadyz.service.consul." ]]; then
              echo "PMMreadyz_check_http check is passing."
              exit 0  
            fi
          else
            echo "PMMreadyz_check_http DNS check is not passing."
          fi
          # If the check is not passing, wait for a short interval and try again
          echo "PMMreadyz_check_http check is not passing. Will retry in 1 second..."
          sleep 1
        done
      fi
  - path: /root/monitor-me.sh
    permissions: "0777"
    content: |
      #!/bin/bash
      # Keep an eye on things
      # watch -n1 -d bash /root/monitor-me.sh
      mysql -D performance_schema -e "select CHANNEL_NAME,MEMBER_ID,MEMBER_HOST,MEMBER_PORT,MEMBER_STATE,MEMBER_COMMUNICATION_STACK From replication_group_members;"
  - path: /root/.my.cnf
    permissions: "0644"
    content: |
      [client]
      password="${mysql_root_password}"
      socket=/data/mysql.sock
      user=root
      verbose
  - path: /etc/my.cnf
    permissions: "0644"
    content: |
      [mysqld]

      # Host specific replication configuration
      #
      report_host = ${name}
      report_port = 3306
      server_id   = ${index}

      # Generic
      #
      bind_address = 0.0.0.0
      datadir=/data
      log_error_verbosity=0
      log-error=/data/mysqld.log
      pid-file=/var/run/mysqld/mysqld.pid
      secure_log_path = /data
      socket=/data/mysql.sock
      loose-validate_password.policy = 0
      #proxy_protocol_networks=10.0.0.0/16

      # Config binary and slow_query logs
      #
      binlog_expire_logs_seconds=604800
      binlog_space_limit=10G
      log_slow_admin_statements=ON
      log_slow_rate_limit=1
      log_slow_rate_type='query'
      log_slow_replica_statements=ON
      log_slow_verbosity=full
      log-bin=/data/percona-group-replication-${index}-bin
      long_query_time=0
      slow_query_log_always_write_time=1
      slow_query_log_use_global_control=all
      slow_query_log=ON
      #max_slowlog_size=3G
      #max_slowlog_files=3

      # Configure statistics
      #
      innodb_monitor_enable=module_index
      performance_schema=ON
      userstat=ON
      
      # Group Replication 
      #
      binlog_checksum = CRC32
      disabled_storage_engines="MyISAM,BLACKHOLE,FEDERATED,ARCHIVE,MEMORY"
      enforce_gtid_consistency = ON
      gtid_mode = ON
      #loose-group_replication_auto_evict_timeout = 60
      loose-group_replication_bootstrap_group=off
      loose-group_replication_certification_loop_chunk_size = 0
      loose-group_replication_certification_loop_sleep_time = 0
      loose-group_replication_flow_control_mode = MAJORITY
      loose-group_replication_group_name="${percona_group_replication_uuid}"
      loose-group_replication_group_seeds= "percona-group-replication-1:33061,percona-group-replication-2:33061,percona-group-replication-3:33061"
      loose-group_replication_local_address= "${name}:33061"
      loose-group_replication_recovery_get_public_key=ON 
      loose-group_replication_start_on_boot=off
      loose-group_replication_xcom_ssl_accept_retries = 10
      loose-group_replication_xcom_ssl_socket_timeout = 0
      plugin_load_add='group_replication.so'
 
      !includedir /etc/my.cnf.d/

  - path: /root/init-mysql.sh
    permissions: "0500"
    content: |
      #!/bin/bash
      set -x
      # This need not be run on the replicas because replication will propagate them everywhere
      provision_users() {
        INITIAL_ROOT_PASSWORD=$(grep "root@localhost:" /data/mysqld.log | tail -n1 | awk '{print $NF}')

        if [ -n "$INITIAL_ROOT_PASSWORD" ]; then
          mysql --socket=/data/mysql.sock --connect-expired-password --verbose -uroot -p$INITIAL_ROOT_PASSWORD -Bse "ALTER USER root@'localhost' IDENTIFIED BY '${mysql_root_password}'; FLUSH PRIVILEGES;"
          if [ $? -ne 0 ]; then
            echo "Error: Failed to change root password"
            #exit 1
          fi
          echo "Password for root@localhost successfully changed"
        else
          echo "Error: Initial root password not found in /data/mysqld.log"
          #exit 1
        fi      
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER 'pmm-admin'@'localhost' IDENTIFIED BY '${mysql_root_password}' WITH MAX_USER_CONNECTIONS 100; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT SELECT, PROCESS, REPLICATION CLIENT, RELOAD, BACKUP_ADMIN ON *.* TO 'pmm-admin'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER 'consul-health-check'@'localhost' IDENTIFIED BY '${mysql_root_password}' WITH MAX_USER_CONNECTIONS 2; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT USAGE ON *.* TO 'consul-health-check'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER replica@'%' IDENTIFIED BY '${mysql_replica_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT REPLICATION SLAVE, CONNECTION_ADMIN, BACKUP_ADMIN, GROUP_REPLICATION_STREAM, REPLICATION CLIENT ON *.* TO replica@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER replica@'localhost' IDENTIFIED BY '${mysql_replica_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT REPLICATION SLAVE, CONNECTION_ADMIN, BACKUP_ADMIN, GROUP_REPLICATION_STREAM, REPLICATION CLIENT ON *.* TO replica@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER proxysql@'%' IDENTIFIED WITH mysql_native_password BY '${proxysql_monitor_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT ALL PRIVILEGES ON *.* TO proxysql@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER 'sysbench-direct-percona-gr'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT ALL PRIVILEGES ON sbtest_direct_gr.* TO 'sysbench-direct-percona-gr'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER 'sysbench-proxysql-percona-gr'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT ALL PRIVILEGES ON sbtest_proxysql_gr.* TO 'sysbench-proxysql-percona-gr'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CREATE USER 'sysbench-haproxy-percona-gr'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;GRANT ALL PRIVILEGES ON sbtest_haproxy_gr.* TO 'sysbench-haproxy-percona-gr'@'%'; FLUSH PRIVILEGES;";
      }

      provision_tables() {
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_direct_gr;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_proxysql_gr;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_haproxy_gr;"
      }

      if [[ "${name}" == "percona-group-replication-1" ]]; then
          # Start GR on primary node
          # If we are running on the primary node (i.e., index == 1), start the GR instance in bootstrap mode.
          provision_users
          provision_tables
          # Persist credentials
          mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CHANGE REPLICATION SOURCE TO SOURCE_USER='replica', SOURCE_PASSWORD='${mysql_replica_password}' FOR CHANNEL 'group_replication_recovery';"
          # Start GR in bootstrap mode
          mysql --defaults-file=/root/.my.cnf -Bse "SET GLOBAL group_replication_bootstrap_group=ON;START GROUP_REPLICATION USER='replica', PASSWORD='${mysql_replica_password}';SET GLOBAL group_replication_bootstrap_group=OFF;"
          
      else
          # Start GR on secondary nodes
          # TODO: Detect when primary node is up and running and start replication
          provision_users
          mysql --defaults-file=/root/.my.cnf -Bse "RESET MASTER;"
          #sleep 30
          #timeout 120 bash -c 'until curl --request 'GET' --insecure https://${pmm_server_endpoint}/v1/readyz ; do sleep 3; done'
          # Persist credentials
          mysql --defaults-file=/root/.my.cnf -Bse "SET SQL_LOG_BIN=0;CHANGE REPLICATION SOURCE TO SOURCE_USER='replica', SOURCE_PASSWORD='${mysql_replica_password}' FOR CHANNEL 'group_replication_recovery';"
          mysql --defaults-file=/root/.my.cnf -Bse "START GROUP_REPLICATION USER='replica', PASSWORD='${mysql_replica_password}';"
      fi
