#cloud-config
fqdn: ${fqdn}
hostname: ${name}
manage_etc_hosts: true
prefer_fqdn_over_hostname: true
preserve_hostname: false
repo_upgrade: all

packages:
  - nmap-ncat
  - jq
  - curl

runcmd:
  - timeout 60 bash -c 'while [ -f /var/run/yum.pid ]; do echo "wait for yum to get unlocked..."; sleep 5; done'
  - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - yum -y install consul
  - systemctl enable consul
  - systemctl start consul
  - yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release setup -y pmm2-client
  - yum -y install pmm2-client
  - pmm-admin config --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - pmm-admin add proxysql --username=admin --password='${proxysql_admin_password}' --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' --server-insecure-tls
  - yum -y install http://yum.puppet.com/puppet8-release-el-7.noarch.rpm
  - yum -y install puppet-agent
  - source /etc/profile.d/puppet-agent.sh ; puppet config set certname ${name} --section main
  - source /etc/profile.d/puppet-agent.sh ; puppet config set server ${pmm_server_host} --section main
  - source /etc/profile.d/puppet-agent.sh ; puppet config set fact_value_length_soft_limit 16384 --section main
  - source /etc/profile.d/puppet-agent.sh ; puppet ssl bootstrap
  - source /etc/profile.d/puppet-agent.sh ; puppet resource service puppet ensure=running enable=true
  - source /etc/profile.d/puppet-agent.sh ; puppet agent --test
  - bash /tmp/waiter.sh readyz
  - pmm-admin config --az="us-east-1f" --region="us-east-1" --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - wget --quiet https://github.com/ncabatoff/process-exporter/releases/download/v0.7.10/process-exporter_0.7.10_linux_386.rpm
  - yum -y install process-exporter_0.7.10_linux_386.rpm
  - service process-exporter start
  - bash /tmp/waiter.sh process-exporter
  - pmm-admin add external --group=processes --listen-port=9256 --environment="prod" --service-name="${name}-processes" --cluster="processes-cluster"
  - percona-release setup -y ps-80
  - yum install -y proxysql2 percona-server-client
  - systemctl start proxysql
  - systemctl enable proxysql
  - bash /tmp/waiter.sh proxysql
  - pmm-admin add proxysql --username=admin --password='${proxysql_admin_password}' --server-insecure-tls --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' --cluster=proxysql-cluster "${name}" "127.0.0.1:6032"

# TODO replace to admin-stats_credentials user

write_files:
  - path: /etc/resolv.conf
    permissions: "0644"
    content: |
      ; generated by #cloud-config
      search ${local_domain} ec2.internal
      options timeout:2 attempts:5
      nameserver 10.0.0.2
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      bootstrap_expect=3
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "bastion"]
      server = true
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/proxysql-service.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "proxysql",
          "name": "proxysql",
          "port": 3306,
          "tags": ["${name}"],
          "checks": [
            {
            "args": ["mysqladmin", "ping", "--host=127.0.0.1", "--port=3306", "--user=sysbench-proxysql-ps", "--password=${percona_server_80_password}"],
            "interval": "3s",
            "Name": "Check for ProxySQL service on 3306 running using mysqladmin",
            "Notes": "Check for ProxySQL service on 3306 running using mysqladmin. Availability of ProxySQL service throughput is reflective of this check.",
            "ServiceID": "proxysql_3306_check_mysqladmin",
            "Success_before_passing": 3,
            "Timeout": "1s"
            },
            {
            "args": ["mysqladmin", "ping", "--host=127.0.0.1", "--port=6032", "--user=admin", "--password=${proxysql_admin_password}"],
            "interval": "3s",
            "Name": "Check for ProxySQL Admin service on 6032 running using mysqladmin",
            "Notes": "Check for ProxySQL Admin service on 6032 running using mysqladmin. Monitoring of ProxySQL Admin is enabled with this service.",
            "ServiceID": "proxysql_6032_check_mysqladmin",
            "Success_before_passing": 3,
            "Timeout": "1s"
            },
            {
            "args": [ "bash", "/tmp/proxysql-admin-script.sh"],
            "interval": "3s",
            "Name": "Check for ProxySQL Admin service on 6032 running a script",
            "Notes": "Check for ProxySQL Admin service on 6032 running a script. Monitoring of ProxySQL Admin is enabled with this service.",
            "ServiceID": "proxysql_6032_check_script",
            "Success_before_passing": 3,
            "Timeout": "1s"
            }
          ]
        }
      }
  - path: /tmp/proxysql-admin-script.sh
    permissions: "0755"
    content: |
      #!/bin/bash

      # Set MySQL server credentials
      MYSQL_USER="admin"
      MYSQL_PASSWORD="${proxysql_admin_password}"
      MYSQL_SOCK="/tmp/proxysql_admin.sock"
      while true ; do
        # Attempt to connect using MySQL Shell with a 2-second timeout
        mysql --user="$MYSQL_USER" --password="$MYSQL_PASSWORD" --socket="$MYSQL_SOCK" --connect-timeout=2 << EOF

        # Verify connection by executing a simple query
        SELECT * FROM runtime_mysql_servers;

      EOF
        
        # Check the exit status of mysql to determine connection success
        if [[ $? -eq 0 ]]; then
          echo "Sucessfully connected to ProxySQL Admin service."
          exit 0
        else
          echo "Connection attempt failed.  Sleeping..."
          sleep 1
        fi

      done
  - path: /etc/consul.d/process-exporter-check-http.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "process-exporter",
          "name": "process-exporter",
          "port": 9256,
          "tags": ["${name}"],
          "checks": [
            {
            "HTTP": "http://${fqdn}:9256/metrics",
            "ID": "process-exporter_check_http",
            "Interval": "3s",
            "Method": "GET",
            "Name": "Check for process-exporter using HTTP",
            "Notes": "Check for process-exporter using HTTP. Monitoring of per-process metrics are enabled with this service.",
            "ServiceID": "process-exporter_check_http",
            "Success_before_passing": 3,
            "Timeout": "5s"
            }
          ]
        }
      }
  - path: /tmp/waiter.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Script to do all the waiting
      service="$1"
      if [[ $service == "process-exporter" ]]; then
        # process-exporter
        while true; do
          # Get the status of the process-exporter_check_http check
          status=$(dig @127.0.0.1 -p 8600 ${name}.process-exporter.service.consul SRV | awk '/SRV.*${fqdn}\.$/ {print $1}')

          if [[ $status == "${name}.process-exporter.service.consul." ]]; then
            echo "process-exporter check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "process-exporter check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "proxysql" ]]; then
        # proxysql
        while true; do
          # Get the status of the Proxysql check
          status=$(dig @127.0.0.1 -p 8600 proxysql.service.consul SRV | awk '/SRV.*proxysql.${environment_name}.local.$/ {print $1}')

          if [[ $status == "proxysql.service.consul." ]]; then
            echo "ProxySQL check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "ProxySQL check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "readyz" ]] ; then
        # PMM readyz
        while true; do
          # Check for DNS :facepalm:
          dnsstatus=$(dig pmm-server.${environment_name}.local A | awk '/A.*10.*$/ {print $1}')

          if [[ $dnsstatus == "pmm-server.${environment_name}.local." ]]; then
            echo "PMMreadyz_check_http DNS check is passing."

            # Get the status of the PMMreadyz_check_http check
            status=$(dig @127.0.0.1 -p 8600 pmmreadyz.service.consul SRV | awk '/SRV.*bastion.${environment_name}.local.$/ {print $1}')
        
            if [[ $status == "pmmreadyz.service.consul." ]]; then
              echo "PMMreadyz_check_http check is passing."
              exit 0  
            fi
          else
            echo "PMMreadyz_check_http DNS check is not passing."
          fi
          # If the check is not passing, wait for a short interval and try again
          echo "PMMreadyz_check_http check is not passing. Will retry in 1 second..."
          sleep 1
        done
      fi
  - path: /etc/proxysql.cnf
    content: |
      datadir="/var/lib/proxysql"
      errorlog="/var/lib/proxysql/proxysql.log"

      mysql_galera_hostgroups=
      (
        {
          # PXC 8 Hostgroups
          writer_hostgroup=1000
          backup_writer_hostgroup=2000
          reader_hostgroup=3000
          offline_hostgroup=4000
          active=1
          max_writers=1
          writer_is_also_reader=0
          max_transactions_behind=0
        }
      )

      mysql_group_replication_hostgroups=
      (
        {
          # Percona Group Replication 8.0 Hostgroups
          writer_hostgroup=10000
          backup_writer_hostgroup=20000
          reader_hostgroup=30000
          offline_hostgroup=40000
          active=1
          max_writers=1
          writer_is_also_reader=0
          max_transactions_behind=0
        }
      )

      mysql_variables=
      {
        threads=1
        max_connections=50000
        default_query_delay=0
        default_query_timeout=36000000
        have_compress=true
        poll_timeout=2000
        interfaces="0.0.0.0:3306"
        default_schema="information_schema"
        stacksize=1048576
        server_version="8.0.19"
        connect_timeout_server=3000
        monitor_username="proxysql"
        monitor_password="${proxysql_monitor_password}"
        monitor_history=600000
        monitor_connect_interval=60000
        monitor_ping_interval=10000
        monitor_read_only_interval=1500
        monitor_read_only_timeout=500
        ping_interval_server_msec=120000
        ping_timeout_server=500
        commands_stats=true
        sessions_sort=true
        connect_retries_on_failure=10
      }
      admin_variables=
      {
        admin_credentials="admin:${proxysql_admin_password}"
        mysql_ifaces="127.0.0.1:6032;/tmp/proxysql_admin.sock"
      }

      mysql_servers =
      (
        { address="percona-server-80-0"         , port=3306 , hostgroup=10, max_connections=200 },
        { address="percona-server-80-1"         , port=3306 , hostgroup=20, max_replication_lag = 30 },
        { address="percona-server-81-0"         , port=3306 , hostgroup=30, max_connections=200 },
        { address="percona-server-81-1"         , port=3306 , hostgroup=40, max_replication_lag = 30 },
        { address="oracle-mysql-83-0"           , port=3306 , hostgroup=50, max_replication_lag = 30 },
        { address="oracle-mysql-83-1"           , port=3306 , hostgroup=60, max_connections=200 },
        { address="percona-xtradb-cluster-0"    , port=3306 , hostgroup=1000 },
        { address="percona-xtradb-cluster-1"    , port=3306 , hostgroup=2000 },
        { address="percona-xtradb-cluster-2"    , port=3306 , hostgroup=2000 },
        { address="percona-group-replication-1" , port=3306 , hostgroup=10000, max_connections=200 },
        { address="percona-group-replication-2" , port=3306 , hostgroup=20000, max_connections=200 },
        { address="percona-group-replication-3" , port=3306 , hostgroup=20000, max_connections=200 },
      )

      mysql_users:
      (
        { username = "sysbench-proxysql-percona-gr" , password = "${percona_group_replication_81_password}" , default_hostgroup = 10000 , active = 1 },
        { username = "sysbench-proxysql-ps" , password = "${percona_server_80_password}" , default_hostgroup = 10 , active = 1 },
        { username = "sysbench-proxysql-ps81" , password = "${percona_server_81_password}" , default_hostgroup = 30 , active = 1 },
        { username = "sysbench-proxysql-pxc" , password = "${percona_xtradb_cluster_80_password}" , default_hostgroup = 1000 , active = 1 },
        { username = "sysbench-prx-oracle-mysql-83" , password = "${oracle_mysql_83_password}" , default_hostgroup = 50 , active = 1 },
      )

      mysql_query_rules:
      (
        { rule_id =  10, destination_hostgroup = 10,    username = "sysbench-proxysql-ps",          match_digest = "^SELECT.*FOR UPDATE", active = 1, apply = 1 },
        { rule_id =  20, destination_hostgroup = 20,    username = "sysbench-proxysql-ps",          match_digest = "^SELECT", active = 1, apply = 1 },
        { rule_id =  30, destination_hostgroup = 30,    username = "sysbench-proxysql-ps81",        match_digest = "^SELECT.*FOR UPDATE", active = 1, apply = 1 },
        { rule_id =  40, destination_hostgroup = 40,    username = "sysbench-proxysql-ps81",        match_digest = "^SELECT", active = 1, apply = 1 },
        { rule_id =  50, destination_hostgroup = 50,    username = "sysbench-prx-oracle-mysql-83",  match_digest = "^SELECT.*FOR UPDATE", active = 1, apply = 1 },
        { rule_id =  60, destination_hostgroup = 60,    username = "sysbench-prx-oracle-mysql-83",  match_digest = "^SELECT", active = 1, apply = 1 },
        { rule_id =  70, destination_hostgroup = 10000, username = "sysbench-proxysql-percona-gr",  match_digest = "^SELECT.*FOR UPDATE", active = 1, apply = 1 },
        { rule_id =  80, destination_hostgroup = 20000, username = "sysbench-proxysql-percona-gr",  match_digest = "^SELECT", active = 1, apply = 1 },
        { rule_id =  90, destination_hostgroup = 1000,  username = "sysbench-proxysql-pxc",         match_digest = "^SELECT.*FOR UPDATE", active = 1, apply = 1 },
        { rule_id = 100, destination_hostgroup = 2000,  username = "sysbench-proxysql-pxc",         match_digest = "^SELECT", active = 1, apply = 1 },
      )

      scheduler=
      (
      )

  - path: /root/.my.cnf
    permissions: "0400"
    content: |
      [client]
      user=admin
      password='${proxysql_admin_password}'
      port=6032
      host=127.1
      prompt='ProxySQL Admin>'
