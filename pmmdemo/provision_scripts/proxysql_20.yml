#cloud-config
preserve_hostname: false
hostname: ${name}
fqdn: ${fqdn}
manage_etc_hosts: true
repo_upgrade: all

packages:
  - nmap-ncat
  - jq
  - curl

runcmd:
  - timeout 60 bash -c 'while [ -f /var/run/yum.pid ]; do echo "wait for yum to get unlocked..."; sleep 5; done'
  - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - yum -y install consul
  - systemctl enable consul
  - systemctl start consul
  - yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release setup -y pmm2-client
  - yum -y install pmm2-client
  - bash /tmp/pmm-server-waiter.sh
  - pmm-admin config --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - percona-release setup -y ps-80
  - yum install -y proxysql2 percona-server-client
  - systemctl start proxysql
  - systemctl enable proxysql
  - pmm-admin add proxysql --username=admin --password='${proxysql_admin_password}' --server-url='https://admin:${pmm_admin_password}@${pmm_server_endpoint}' --server-insecure-tls
  - wget https://github.com/ncabatoff/process-exporter/releases/download/v0.7.10/process-exporter_0.7.10_linux_386.rpm
  - yum -y install process-exporter_0.7.10_linux_386.rpm
  - service process-exporter start
  - pmm-admin add external --group=processes --listen-port=9256 --environment="${environment_name}" --service-name="${name}-processes"

# TODO replace to admin-stats_credentials user

write_files:
  - path: /etc/resolv.conf
    permissions: "0644"
    content: |
      ; generated by #cloud-config
      search ${local_domain} ec2.internal
      options timeout:2 attempts:5
      nameserver 10.0.0.2
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      bootstrap_expect=3
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "bastion"]
      server = true
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/proxysql-service.json
    permissions: "0644"
    content: |
      {
        "service": {
          "id": "proxysql",
          "name": "proxysql",
          "tags": ["${name}"],
          "address": "",
          "port": 3306,
          "checks": [
            {
        "args": ["mysqladmin", "ping", "--host=127.0.0.1", "--port=3306", "--user=sysbench-proxysql-ps", "--password=${percona_server_80_password}"],
              "interval": "10s"
            }
          ]
        }
      }
  - path: /tmp/pmm-server-waiter.sh
    permissions: "0755"
    content: |
      #!/bin/bash

      while true; do
      # Get the status of the PMMreadyz_check_http check
      status=$(dig @127.0.0.1 -p 8600 pmmreadyz.service.consul SRV  | grep bastion | awk '/A/ {print $1}')
      
      if [[ $status == "bastion.node.dc1.consul." ]]; then
        echo "PMMreadyz_check_http check is passing."
        exit 0
      fi

      # If the check is not passing, wait for a short interval and try again
      echo "PMMreadyz_check_http check is not passing. Will retry in 5 seconds..."
      sleep 5
      done
  - path: /etc/proxysql.cnf
    content: |
      datadir="/var/lib/proxysql"
      errorlog="/var/lib/proxysql/proxysql.log"

      mysql_galera_hostgroups=
      (
        {
          # PXC 8 Hostgroups
          writer_hostgroup=1000
          backup_writer_hostgroup=2000
          reader_hostgroup=3000
          offline_hostgroup=4000
          active=1
          max_writers=1
          writer_is_also_reader=0
          max_transactions_behind=0
        }
      )

      mysql_group_replication_hostgroups=
      (
        {
          # Percona Group Replication 8.0 Hostgroups
          writer_hostgroup=10000
          backup_writer_hostgroup=20000
          reader_hostgroup=30000
          offline_hostgroup=40000
          active=1
          max_writers=1
          writer_is_also_reader=0
          max_transactions_behind=0
        }
      )

      mysql_variables=
      {
        threads=1
        max_connections=50000
        default_query_delay=0
        default_query_timeout=36000000
        have_compress=true
        poll_timeout=2000
        interfaces="0.0.0.0:3306"
        default_schema="information_schema"
        stacksize=1048576
        server_version="8.0.19"
        connect_timeout_server=3000
        monitor_username="proxysql"
        monitor_password="${proxysql_monitor_password}"
        monitor_history=600000
        monitor_connect_interval=60000
        monitor_ping_interval=10000
        monitor_read_only_interval=1500
        monitor_read_only_timeout=500
        ping_interval_server_msec=120000
        ping_timeout_server=500
        commands_stats=true
        sessions_sort=true
        connect_retries_on_failure=10
      }
      admin_variables=
      {
        admin_credentials="admin:${proxysql_admin_password}"
        mysql_ifaces="127.0.0.1:6032;/tmp/proxysql_admin.sock"
      }

      mysql_servers =
      (
        { address="percona-server-80-0" , port=3306 , hostgroup=10, max_connections=200 },
        { address="percona-server-80-1" , port=3306 , hostgroup=20, max_replication_lag = 30 },
        { address="percona-server-81-0" , port=3306 , hostgroup=100, max_connections=200 },
        { address="percona-server-81-1" , port=3306 , hostgroup=200, max_replication_lag = 30 },
        { address="percona-xtradb-cluster-0" , 	port=3306 , hostgroup=1000 },
        { address="percona-xtradb-cluster-1" , 	port=3306 , hostgroup=2000 },
        { address="percona-xtradb-cluster-2" , 	port=3306 , hostgroup=2000 },
        { address="percona-group-replication-1" , port=3306 , hostgroup=10000, max_connections=200 },
        { address="percona-group-replication-2" , port=3306 , hostgroup=20000, max_connections=200 },
        { address="percona-group-replication-3" , port=3306 , hostgroup=20000, max_connections=200 },
      )

      mysql_users:
      (
        { username = "sysbench-proxysql-pxc" , password = "${percona_xtradb_cluster_80_password}" , default_hostgroup = 1000 , active = 1 },
        { username = "sysbench-proxysql-ps" , password = "${percona_server_80_password}" , default_hostgroup = 10 , active = 1 },
        { username = "sysbench-proxysql-ps81" , password = "${percona_server_81_password}" , default_hostgroup = 100 , active = 1 },
        { username = "sysbench-proxysql-percona-gr" , password = "${percona_group_replication_81_password}" , default_hostgroup = 10000 , active = 1 },


      )

      mysql_query_rules:
      (
        { rule_id = 10, username = "sysbench-proxysql-ps", destination_hostgroup = 10, active = 1, match_digest = "^SELECT.*FOR UPDATE", apply = 1},
        { rule_id = 20, username = "sysbench-proxysql-ps", destination_hostgroup = 20, active = 1, match_digest = "^SELECT", apply = 1},
        { rule_id = 30, username = "sysbench-proxysql-pxc", destination_hostgroup = 1000, active = 1, match_digest = "^SELECT.*FOR UPDATE", apply = 1},
        { rule_id = 40, username = "sysbench-proxysql-pxc", destination_hostgroup = 2000, active = 1, match_digest = "^SELECT", apply = 1},
        { rule_id = 50, username = "sysbench-proxysql-ps81", destination_hostgroup = 100, active = 1, match_digest = "^SELECT.*FOR UPDATE", apply = 1},
        { rule_id = 60, username = "sysbench-proxysql-ps81", destination_hostgroup = 200, active = 1, match_digest = "^SELECT", apply = 1},
        { rule_id = 70, username = "sysbench-proxysql-percona-gr", destination_hostgroup = 10000, active = 1, match_digest = "^SELECT.*FOR UPDATE", apply = 1},
        { rule_id = 80, username = "sysbench-proxysql-percona-gr", destination_hostgroup = 20000, active = 1, match_digest = "^SELECT", apply = 1},
      )

      scheduler=
      (
      )

  - path: /root/.my.cnf
    permissions: "0400"
    content: |
      [client]
      user=admin
      password='${proxysql_admin_password}'
      port=6032
      host=127.1
      prompt='ProxySQL Admin>'
