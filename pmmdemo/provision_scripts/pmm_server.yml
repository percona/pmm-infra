#cloud-config
preserve_hostname: false
hostname: ${name}
fqdn: ${fqdn}
manage_etc_hosts: true
repo_upgrade: all

packages:
  - curl
  - lsof
  - nmap
  - htop
  - mc
  - jq
  - docker
  - git
  - yum-utils

bootcmd:
  - while [[ ! -b $(readlink -f /dev/nvme1n1) ]]; do echo "waiting for the disk..."; sleep 5; done
  - mkfs.xfs -L data /dev/nvme1n1
  - mkdir -p /var/lib/docker/

mounts:
  - ["/dev/nvme1n1", "/var/lib/docker/", "xfs", "defaults,noatime", "0", "2"]

runcmd:
  - systemctl enable docker
  - service docker start
  - usermod -a -G docker ec2-user
  - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - yum -y install consul
  - systemctl enable consul
  - bash /root/consul-advertise-addr.sh
  - systemctl start consul
  - yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release setup -y ps-80
  - yum -y install percona-server-client
  - docker pull percona/pmm-server:2
  - docker volume create pmm-data || true
  - docker run -d -p 443:443 -p 80:80 -v pmm-data:/srv --name pmm-server --restart always percona/pmm-server:2
  - sleep 20s
  - docker exec pmm-server change-admin-password ${pmm_admin_pass}
  - "echo '${pmm_admin_pass}' > /root/pmm-admin-password"
  - chmod 400 /root/pmm-admin-password
  - docker cp /root/ansible.yml pmm-server:/root/ansible.yml
  - docker exec pmm-server ansible-playbook -v /root/ansible.yml
  - docker exec pmm-server supervisorctl restart grafana
  - yum -y install https://github.com/openark/orchestrator/releases/download/v3.2.6/orchestrator-3.2.6-1.x86_64.rpm
  - systemctl enable orchestrator.service
  - systemctl start orchestrator.service
write_files:
  - path: /etc/orchestrator.conf.json
    permissions: "0664"
    group: "wheel"
    content: |
      {
      "Debug": true,
      "EnableSyslog": false,
      "ListenAddress": ":3000",
      "MySQLTopologyUser": "orc_client_user",
      "MySQLTopologyPassword": "${proxysql_monitor_password}",
      "MySQLTopologyCredentialsConfigFile": "",
      "MySQLTopologySSLPrivateKeyFile": "",
      "MySQLTopologySSLCertFile": "",
      "MySQLTopologySSLCAFile": "",
      "MySQLTopologySSLSkipVerify": true,
      "MySQLTopologyUseMutualTLS": false,
      "BackendDB": "sqlite",
      "SQLite3DataFile": "/usr/local/orchestrator/orchestrator.sqlite3",
      "MySQLConnectTimeoutSeconds": 1,
      "DefaultInstancePort": 3306,
      "DiscoverByShowSlaveHosts": true,
      "InstancePollSeconds": 5,
      "DiscoveryIgnoreReplicaHostnameFilters": [
        "a_host_i_want_to_ignore[.]example[.]com",
        ".*[.]ignore_all_hosts_from_this_domain[.]example[.]com",
        "a_host_with_extra_port_i_want_to_ignore[.]example[.]com:3307"
      ],
      "UnseenInstanceForgetHours": 1,
      "SnapshotTopologiesIntervalHours": 0,
      "InstanceBulkOperationsWaitTimeoutSeconds": 10,
      "HostnameResolveMethod": "default",
      "MySQLHostnameResolveMethod": "@@hostname",
      "SkipBinlogServerUnresolveCheck": true,
      "ExpiryHostnameResolvesMinutes": 60,
      "RejectHostnameResolvePattern": "",
      "ReasonableReplicationLagSeconds": 10,
      "ProblemIgnoreHostnameFilters": [],
      "VerifyReplicationFilters": false,
      "ReasonableMaintenanceReplicationLagSeconds": 20,
      "CandidateInstanceExpireMinutes": 60,
      "AuditLogFile": "",
      "AuditToSyslog": false,
      "RemoveTextFromHostnameDisplay": ".pmmdemodev.local:3306",
      "ReadOnly": false,
      "AuthenticationMethod": "",
      "HTTPAuthUser": "",
      "HTTPAuthPassword": "",
      "AuthUserHeader": "",
      "PowerAuthUsers": [
        "*"
      ],
      "ClusterNameToAlias": {
        "127.0.0.1": "test suite"
      },
      "ReplicationLagQuery": "",
      "DetectClusterAliasQuery": "SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(@@hostname, '.', 1), '-', 3);",
      "DetectClusterDomainQuery": "",
      "DetectInstanceAliasQuery": "",
      "DetectPromotionRuleQuery": "",
      "DataCenterPattern": "[.]([^.]+)[.][^.]+[.]pmmdemodev[.]local",
      "PhysicalEnvironmentPattern": "[.]([^.]+[.][^.]+)[.]pmmdemodev[.]local",
      "PromotionIgnoreHostnameFilters": [],
      "DetectSemiSyncEnforcedQuery": "",
      "ServeAgentsHttp": false,
      "AgentsServerPort": ":3001",
      "AgentsUseSSL": false,
      "AgentsUseMutualTLS": false,
      "AgentSSLSkipVerify": false,
      "AgentSSLPrivateKeyFile": "",
      "AgentSSLCertFile": "",
      "AgentSSLCAFile": "",
      "AgentSSLValidOUs": [],
      "UseSSL": false,
      "UseMutualTLS": false,
      "SSLSkipVerify": false,
      "SSLPrivateKeyFile": "",
      "SSLCertFile": "",
      "SSLCAFile": "",
      "SSLValidOUs": [],
      "URLPrefix": "",
      "StatusEndpoint": "/api/status",
      "StatusSimpleHealth": true,
      "StatusOUVerify": false,
      "AgentPollMinutes": 60,
      "UnseenAgentForgetHours": 6,
      "StaleSeedFailMinutes": 60,
      "SeedAcceptableBytesDiff": 8192,
      "PseudoGTIDPattern": "",
      "PseudoGTIDPatternIsFixedSubstring": false,
      "PseudoGTIDMonotonicHint": "asc:",
      "DetectPseudoGTIDQuery": "",
      "BinlogEventsChunkSize": 10000,
      "SkipBinlogEventsContaining": [],
      "ReduceReplicationAnalysisCount": true,
      "FailureDetectionPeriodBlockMinutes": 60,
      "FailMasterPromotionOnLagMinutes": 0,
      "RecoveryPeriodBlockSeconds": 3600,
      "RecoveryIgnoreHostnameFilters": [],
      "RecoverMasterClusterFilters": [
        "_master_pattern_"
      ],
      "RecoverIntermediateMasterClusterFilters": [
        "_intermediate_master_pattern_"
      ],
      "OnFailureDetectionProcesses": [
        "echo 'Detected {failureType} on {failureCluster}. Affected replicas: {countSlaves}' >> /tmp/recovery.log"
      ],
      "PreGracefulTakeoverProcesses": [
        "echo 'Planned takeover about to take place on {failureCluster}. Master will switch to read_only' >> /tmp/recovery.log"
      ],
      "PreFailoverProcesses": [
        "echo 'Will recover from {failureType} on {failureCluster}' >> /tmp/recovery.log"
      ],
      "PostFailoverProcesses": [
        "echo '(for all types) Recovered from {failureType} on {failureCluster}. Failed: {failedHost}:{failedPort}; Successor: {successorHost}:{successorPort}' >> /tmp/recovery.log"
      ],
      "PostUnsuccessfulFailoverProcesses": [],
      "PostMasterFailoverProcesses": [
        "echo 'Recovered from {failureType} on {failureCluster}. Failed: {failedHost}:{failedPort}; Promoted: {successorHost}:{successorPort}' >> /tmp/recovery.log"
      ],
      "PostIntermediateMasterFailoverProcesses": [
        "echo 'Recovered from {failureType} on {failureCluster}. Failed: {failedHost}:{failedPort}; Successor: {successorHost}:{successorPort}' >> /tmp/recovery.log"
      ],
      "PostGracefulTakeoverProcesses": [
        "echo 'Planned takeover complete' >> /tmp/recovery.log"
        pmm-admin annotate --tags "$(echo '{successorHost}')" "$(echo '{failureType} on {failureCluster}. Failed: {failedHost}:{failedPort}; Successor: {successorHost}:{successorPort}')"
      ],
      "CoMasterRecoveryMustPromoteOtherCoMaster": true,
      "DetachLostSlavesAfterMasterFailover": true,
      "ApplyMySQLPromotionAfterMasterFailover": true,
      "PreventCrossDataCenterMasterFailover": false,
      "PreventCrossRegionMasterFailover": false,
      "MasterFailoverDetachReplicaMasterHost": false,
      "MasterFailoverLostInstancesDowntimeMinutes": 0,
      "PostponeReplicaRecoveryOnLagMinutes": 0,
      "OSCIgnoreHostnameFilters": [],
      "GraphiteAddr": "",
      "GraphitePath": "",
      "GraphiteConvertHostnameDotsToUnderscores": true,
      "KVClusterMasterPrefix": "mysql/master",
      "ConsulAddress": "127.0.0.1:8500",
      "ConsulKVStoreProvider": "consul-txn",
      "ConsulAclToken": ""
      }
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      bootstrap_expect=3
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "proxysql"]
      server = true
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/pmm-readyz.json
    permissions: "0644"
    content: |
      {
        "service": {
          "id": "PMMreadyz",
          "name": "PMMreadyz",
          "checks": [
            {
              "args": ["curl", "--silent", "--insecure", "https://pmm-server/v1/readyz"],
            "interval": "10s"
            }
          ]
        }
      }
  - path: /root/consul-advertise-addr.sh
    permissions: "0700"
    content: |
      #!/bin/bash
      my_private_ip=$(ip a sh eth0 | awk '/inet / {print $2}' | awk -F '/' '{print $1}')
      echo "advertise_addr = \"$my_private_ip\"" >> /etc/consul.d/consul.hcl
  - path: /root/ansible.yml
    permissions: "0400"
    content: |
      ---
      # This playbook contains tasks executed for pmmdemo
      - hosts: localhost
        become: yes
        gather_facts: yes
        tasks:
          - name: Set Google Analytics ID
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: analytics
              option: google_analytics_ua_id
              value: ${google_analytics_id}

          - name: Enable anonymous access (readonly)
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: auth.anonymous
              option: enabled
              value: "true"

          - name: Set domain name
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: server
              option: domain
              value: ${full_domain}

          - name: Disable telemetry
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: analytics
              option: reporting_enabled
              value: "false"

          - name: Disable updates
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: analytics
              option: check_for_updates
              value: "false"

          - name: Enable alerting
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: alerting
              option: enabled
              value: "false"

          - name: Enable secure cookies
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: security
              option: cookie_secure
              value: "true"

          - name: Set auth cookie name
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: auth
              option: login_cookie_name
              value: pmm_session

          - name: Set max login duration
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: auth
              option: login_maximum_lifetime_duration
              value: 7d

          - name: Enable unified alerting
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: unified_alerting
              option: enabled
              value: "true"

          - name: Set instance name
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: DEFAULT
              option: instance_name
              value: "PMM Demo"

          - name: Enable panel alpha feature
            ini_file:
              dest: /etc/grafana/grafana.ini
              section: panels
              option: enable_alpha
              value: "true"

          - name: Configure PMM's connection to the Portal
            when: ${oauth_enable}
            block:
            # TODO use loop instead block here
            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: enabled
                value: "true"

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: name
                value: "Percona Account"

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: client_id
                value: ${oauth_client_id}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: client_secret
                value: ${oauth_secret}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: scopes
                value: ${oauth_scopes}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: auth_url
                value: ${oauth_url}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: token_url
                value: ${oauth_token_url}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: api_url
                value: ${oauth_api_url}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth.generic_oauth
                option: role_attribute_path
                value: ${oauth_role_attribute_path}

            - ini_file:
                dest: /etc/grafana/grafana.ini
                section: auth
                option: signout_redirect_url
                value: ${oauth_signout_redirect_url}
  - path: /root/volume-backup.sh
    permissions: "0755"
    content: |
      #!/bin/bash -xe

      docker run --rm --volumes-from pmm-data -v /root/backup:/backup busybox tar czvf /backup/backup-$(date "+%F").tar /srv
  - path: /root/upgrade-pmm.sh
    permissions: "0755"
    content: |
      #!/bin/bash -xe

      docker pull percona/pmm-server:2

      docker stop pmm-server
      docker rm pmm-server

      docker run -d -p 443:443 -p 80:80 --volumes-from pmm-data --name pmm-server --restart always percona/pmm-server:2
      echo "PMM Server has been successfully upgraded to the next version. Don't forget to purge the old image."

      bash /root/apply-custom-settings.sh
  - path: /root/apply-custom-settings.sh
    permissions: "0755"
    content: |
      #!/bin/bash -xe

      echo "Running the ansible script..."
      docker cp /root/ansible.yml pmm-server:/root/ansible.yml
      docker exec pmm-server ansible-playbook -v /root/ansible.yml
      docker exec pmm-server supervisorctl restart grafana
      echo "PMM settings updated."
