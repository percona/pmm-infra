#cloud-config
fqdn: ${fqdn}
hostname: ${name}
manage_etc_hosts: true
prefer_fqdn_over_hostname: true
preserve_hostname: false
repo_upgrade: all

bootcmd:
  - while [[ ! -b $(readlink -f /dev/nvme1n1) ]]; do echo "waiting for the disk..."; sleep 5; done
  - mkfs.xfs -L data /dev/nvme1n1
  - mkdir -p /data

runcmd:
  - dnf config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - dnf -y install consul
  - systemctl enable consul
  - systemctl start consul
  - dnf -y install https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release enable-only pxc-80 release
  - percona-release enable tools release
  - dnf -y install percona-xtradb-cluster
  - percona-release enable tools pmm2-client
  - percona-release setup -y pmm3-client
  - dnf -y install pmm-client
  - systemctl enable mysql
  - systemctl start mysql
  - bash /root/init-mysql.sh
  - curl -s https://raw.githubusercontent.com/percona/pmm-infra/pmmdemo/provision_scripts/scripts/waiter.sh -o /usr/local/bin/waiter.sh
  - chmod 0755 /usr/local/bin/waiter.sh
  - bash /usr/local/bin/waiter.sh readyz ${name} ${fqdn} ${environment_name}
  - pmm-admin config --az="us-east-1f" --region="us-east-1" --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - bash /usr/local/bin/waiter.sh mysql
  - pmm-admin add mysql --metrics-mode=push --username=pmm-admin --password='${mysql_root_password}' --cluster='pxc-80-cluster' --replication-set='pxc-80-cluster' --environment='prod' --query-source=slowlog --service-name="${name}-mysql"
  - dnf -y install https://github.com/ncabatoff/process-exporter/releases/download/v0.8.5/process-exporter_0.8.5_linux_amd64.rpm
  - bash /usr/local/bin/waiter.sh process-exporter ${name} ${fqdn} ${environment_name}
  - pmm-admin add external --group=processes --listen-port=9256 --environment="prod" --service-name="${name}-processes" --cluster="processes-cluster"

mounts:
  - ["/dev/nvme1n1", "/data", "xfs", "defaults,noatime", "0", "2"]

write_files:
  - path: /etc/resolv.conf
    permissions: "0644"
    content: |
      ; generated by #cloud-config
      search ${local_domain} ec2.internal
      options timeout:2 attempts:5
      nameserver 10.0.0.2
  - path: /usr/local/bin/consul-check-mysql.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Set MySQL server credentials
      MYSQL_USER="consul-health-check"
      MYSQL_PASSWORD="${mysql_root_password}"
      MYSQL_SOCKET="/var/lib/mysql/mysql.sock"
      while true ; do
        # Attempt to connect using MySQL Shell with a 2-second timeout
        mysql --socket="$MYSQL_SOCKET" --user="$MYSQL_USER" --password="$MYSQL_PASSWORD" --connect-timeout=2 << EOF

        SELECT 1;

      EOF

        # Check the exit status of mysqlsh to determine connection success
        if [[ $? -eq 0 ]]; then
          echo "Successfully connected to MySQL service."
          exit 0
        else
          echo -n "Connection attempt to MySQL service failed. Sleeping 1 second... "
          date
          sleep 1
        fi

      done
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "bastion"]
      server = false
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/percona-xtradb-cluster-service.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "percona-xtradb-cluster",
          "name": "percona-xtradb-cluster",
          "port": 3306,
          "tags": ["${name}"],
          "checks": [
            {
            "ID": "percona-xtradb-cluster_check_script",
            "Interval": "3s",
            "Name": "Check for MySQL running on ${name}",
            "Notes": "Check for MySQL running on ${name}. Monitoring of MySQL metrics are enabled with this service.",
            "ServiceID": "percona-xtradb-cluster_check_script",
            "Success_before_passing": 3,
            "Timeout": "3s",
            "args": [ "bash", "/usr/local/bin/consul-check-mysql.sh" ]
            }
          ]
        }
      }
  - path: /etc/consul.d/process-exporter-check-http.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "process-exporter",
          "name": "process-exporter",
          "port": 9256,
          "tags": ["${name}"],
          "checks": [
            {
            "HTTP": "http://${fqdn}:9256/metrics",
            "ID": "process-exporter_check_http",
            "Interval": "3s",
            "Method": "GET",
            "Name": "Check for process-exporter using HTTP",
            "Notes": "Check for process-exporter using HTTP. Monitoring of per-process metrics are enabled with this service.",
            "ServiceID": "process-exporter_check_http",
            "Success_before_passing": 3,
            "Timeout": "5s"
            }
          ]
        }
      }
  - path: /usr/local/bin/waiter.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Script to do all the waiting
      # File contents will be downloaded from githubcontent.com
  - path: /root/.my.cnf
    permissions: "0644"
    content: |
      [client]
      user=root
      password="${mysql_root_password}"

  - path: /etc/my.cnf
    permissions: "0644"
    content: |
      [mysqld]

      # Host specific replication configuration
      #
      report_host = ${fqdn}
      report_port = 3306
      server_id   = ${index}


      # Generic
      #
      datadir=/data
      socket=/var/lib/mysql/mysql.sock
      log-error=/var/log/mysqld.log
      pid-file=/var/run/mysqld/mysqld.pid
      log_error_verbosity=3
      log_error_suppression_list='MY-013360'
      #proxy_protocol_networks=10.0.0.0/16
      loose-validate_password.policy = 0


      # Config binary and slow_query logs
      #
      log-bin=percona-xtradb-cluster-{index}-bin
      binlog_format=ROW
      binlog_expire_logs_seconds=86400
      binlog_space_limit=10G

      slow_query_log=ON
      slow_query_log_always_write_time=1
      slow_query_log_use_global_control=all
      log_slow_rate_limit=1
      log_slow_rate_type='query'
      log_slow_admin_statements=ON
      log_slow_replica_statements=ON
      log_slow_verbosity=full
      long_query_time=0
      #max_slowlog_size=3G
      #max_slowlog_files=3

      # Configure statistics
      #
      userstat=ON
      performance_schema=ON
      innodb_monitor_enable=module_index

      # wsrep
      #

      # Cluster name
      wsrep_cluster_name=pxc-80-cluster

      # If wsrep_node_name is not specified,  then system hostname will be used
      wsrep_node_name=${name}

      # Cluster connection URL contains IPs of nodes. If no IP is found, this implies that a new cluster needs to be created, in order to do that you need to bootstrap this node
      wsrep_cluster_address=gcomm://percona-xtradb-cluster-0,percona-xtradb-cluster-1,percona-xtradb-cluster-2

      # Path to Galera library
      wsrep_provider=/usr/lib64/galera4/libgalera_smm.so

      # Slave thread to use
      wsrep_applier_threads=8

      # Enable logging of conflicting statements
      wsrep_log_conflicts

      # This changes how InnoDB autoincrement locks are managed and is a requirement for Galera
      innodb_autoinc_lock_mode=2

      # pxc_strict_mode allowed values: DISABLED,PERMISSIVE,ENFORCING,MASTER
      pxc_strict_mode=ENFORCING

      # disable encryption of cluster traffic
      pxc_encrypt_cluster_traffic=OFF

      # SST method
      wsrep_sst_method=xtrabackup-v2

      [sst]
      wsrep_debug=ON

      !includedir /etc/my.cnf.d/

  - path: /root/init-mysql.sh
    permissions: "0500"
    content: |
      #!/bin/bash

      # This need not be run on the replicas because replication will propagate them everywhere
      provision_users() {
        INITIAL_ROOT_PASSWORD=$(grep "root@localhost:" /var/log/mysqld.log | tail -n1 | awk '{print $NF}')

        if [ -n "$INITIAL_ROOT_PASSWORD" ]; then
          mysql --connect-expired-password -uroot -p$INITIAL_ROOT_PASSWORD -Bse "ALTER USER root@'localhost' IDENTIFIED BY '${mysql_root_password}'; FLUSH PRIVILEGES;"
          echo "Password for root@localhost successfully changed"
          break
        else
          echo "Error: Initial root password not found in /var/log/mysqld.log"
          exit 1
        fi      

        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'pmm-admin'@'localhost' IDENTIFIED WITH mysql_native_password BY '${mysql_root_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT SELECT, PROCESS, REPLICATION CLIENT, RELOAD, BACKUP_ADMIN ON *.* TO 'pmm-admin'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'consul-health-check'@'localhost' IDENTIFIED BY '${mysql_root_password}' WITH MAX_USER_CONNECTIONS 2; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT USAGE ON *.* TO 'consul-health-check'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_direct;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_proxysql;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_haproxy;"
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest.* TO 'pmm-admin'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-direct-pxc'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_direct.* TO 'sysbench-direct-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-proxysql-pxc'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_proxysql.* TO 'sysbench-proxysql-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-haproxy-pxc'@'%' IDENTIFIED WITH mysql_native_password BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_haproxy.* TO 'sysbench-haproxy-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER proxysql@'%' IDENTIFIED WITH mysql_native_password BY '${proxysql_monitor_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON *.* TO proxysql@'%'; FLUSH PRIVILEGES;";
      }


      if [[ "${name}" == "percona-xtradb-cluster-0" ]]; then
          # If we are running on the primary node (i.e., index == 0), start the PXC instance in bootstrap mode.
          systemctl start mysql@bootstrap

          provision_users

          # Check 600 times (1200sec or 20min) if PXC is ready (all 3 nodes have joined and cluster is OPERATIONAL). Switch from bootstrapping to regular operation
          for (( i=1 ; i<=600 ; i++ )); do
              if $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_evs_state';" | grep "OPERATIONAL" >/dev/null) && \
                $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_cluster_size';" | grep "3" >/dev/null)        && \
                $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_local_state_comment';" | grep "Synced" >/dev/null); then

                  # Proceed only if we have 10 successfull checks. Avoid trigger on first occurence.
                  if [[ "$${CHECK_COUNT:=1}" -ge "10" ]]; then
                      # Stop bootstrap mode
                      systemctl stop mysql@bootstrap

                      # Start primary node and break the loop
                      systemctl start mysql
                      break
                  else
                      # Increment + sleep
                      CHECK_COUNT="$(($${CHECK_COUNT:=1} + 1))"
                      sleep 1
                  fi;
              else
                  # Wait for 2sec until next retry
                  echo "retry $${i}/600"
                  sleep 2
              fi
          done
      else
          systemctl start mysql

          # Check 300 times (600sec/10min) if Primary is ready (equalize differences in deployment times - Primary can finish pre_provisoining last).
          for (( i=1 ; i<=300 ; i++ )); do
              if $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_evs_state';" | grep "OPERATIONAL" >/dev/null); then
                  # Start secondary node and brake the loop
                  systemctl start mysql
                  break
              else
                  # Wait for 2sec until next retry
                  echo "retry $${i}/300"
                  sleep 2
              fi
          done
      fi
