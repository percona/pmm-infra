#cloud-config
fqdn: ${fqdn}
hostname: ${name}
manage_etc_hosts: true
prefer_fqdn_over_hostname: true
preserve_hostname: false
package_upgrade: true
package_update: true

packages:
  - bind-utils
  - curl

bootcmd:
  - while [[ ! -b $(readlink -f /dev/nvme1n1) ]]; do echo "waiting for the disk..."; sleep 5; done
  - mkfs.xfs -L data /dev/nvme1n1
  - mkdir -p /data

mounts:
  - ["/dev/nvme1n1", "/data", "xfs", "defaults,noatime", "0", "2"]

runcmd:
  - dnf config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  - dnf -y install consul
  - systemctl enable consul && systemctl start consul
  - dnf -y install https://repo.percona.com/yum/percona-release-latest.noarch.rpm
  - percona-release setup -y pxc-80
  - PERCONA_TELEMETRY_DISABLE=1 dnf -y install percona-xtradb-cluster
  - percona-release setup -y pmm3-client
  - dnf -y install pmm-client
  - echo '!includedir /etc/my.cnf.d/' >> /etc/my.cnf
  - chown mysql:mysql /data/
  - semanage fcontext -a -t mysqld_db_t "/data(/.*)?" && restorecon -Rv /data
  - for port in 4444 4567 4568; do semanage port -a -t mysqld_port_t -p tcp $port; done
  - systemctl enable mysql.service
  - systemctl start mysql.service
  - systemctl status mysql.service
  - bash /root/init-mysql.sh
  - bash /usr/local/bin/waiter.sh readyz
  - pmm-admin config --az="us-east-1f" --region="us-east-1" --metrics-mode=push --force --server-insecure-tls --server-url='https://admin:${pmm_password}@${pmm_server_endpoint}' ${fqdn} generic ${name}
  - bash /usr/local/bin/waiter.sh mysql
  - pmm-admin add mysql --metrics-mode=push --username=pmm-admin --password='${mysql_root_password}' --cluster='pxc-80-cluster' --replication-set='pxc-80-cluster' --environment='prod' --query-source=slowlog --service-name="${name}-mysql"
  - dnf -y install https://github.com/ncabatoff/process-exporter/releases/download/v0.8.5/process-exporter_0.8.5_linux_amd64.rpm
  - bash /usr/local/bin/waiter.sh process-exporter
  - pmm-admin add external --group=processes --listen-port=9256 --environment="prod" --service-name="${name}-processes" --cluster="processes-cluster"

write_files:
  - path: /etc/resolv.conf
    permissions: "0644"
    content: |
      ; generated by #cloud-config
      search ${local_domain} ec2.internal
      options timeout:2 attempts:5
      nameserver 10.0.0.2
  - path: /usr/local/bin/consul-check-mysql.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Set MySQL server credentials
      MYSQL_USER="consul-health-check"
      MYSQL_PASSWORD="${mysql_root_password}"
      MYSQL_SOCKET="/var/lib/mysql/mysql.sock"
      while true ; do
        # Attempt to connect using MySQL Shell with a 2-second timeout
        mysql --socket="$MYSQL_SOCKET" --user="$MYSQL_USER" --password="$MYSQL_PASSWORD" --connect-timeout=2 -e "SELECT 1"
        # Check the exit status of mysqlsh to determine connection success
        if [[ $? -eq 0 ]]; then
          echo "Successfully connected to MySQL service."
          exit 0
        else
          echo -n "Connection attempt to MySQL service failed. Sleeping 1 second... "
          date
          sleep 1
        fi
      done
  - path: /etc/consul.d/consul.hcl
    permissions: "0644"
    content: |
      bind_addr = "0.0.0.0"
      client_addr = "0.0.0.0"
      data_dir = "/opt/consul"
      enable_local_script_checks = true
      node_name="${name}"
      retry_join = ["pmm-server", "sysbench", "bastion"]
      server = false
      ui_config{
        enabled = true
      }
  - path: /etc/consul.d/percona-xtradb-cluster-service.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "percona-xtradb-cluster",
          "name": "percona-xtradb-cluster",
          "port": 3306,
          "tags": ["${name}"],
          "checks": [
            {
            "ID": "percona-xtradb-cluster_check_script",
            "Interval": "3s",
            "Name": "Check for MySQL running on ${name}",
            "Notes": "Check for MySQL running on ${name}. Monitoring of MySQL metrics are enabled with this service.",
            "ServiceID": "percona-xtradb-cluster_check_script",
            "Success_before_passing": 3,
            "Timeout": "3s",
            "args": [ "bash", "/usr/local/bin/consul-check-mysql.sh" ]
            }
          ]
        }
      }
  - path: /etc/consul.d/process-exporter-check-http.json
    permissions: "0644"
    content: |
      {
        "service": {
          "address": "${fqdn}",
          "id": "process-exporter",
          "name": "process-exporter",
          "port": 9256,
          "tags": ["${name}"],
          "checks": [
            {
            "HTTP": "http://${fqdn}:9256/metrics",
            "ID": "process-exporter_check_http",
            "Interval": "3s",
            "Method": "GET",
            "Name": "Check for process-exporter using HTTP",
            "Notes": "Check for process-exporter using HTTP. Monitoring of per-process metrics are enabled with this service.",
            "ServiceID": "process-exporter_check_http",
            "Success_before_passing": 3,
            "Timeout": "5s"
            }
          ]
        }
      }
  - path: /usr/local/bin/waiter.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Script to do all the waiting
      service="$1"
      if [[ $service == "process-exporter" ]]; then
        # process-exporter
        while true; do
          # Get the status of the process-exporter_check_http check
          status=$(dig @127.0.0.1 -p 8600 ${name}.process-exporter.service.consul SRV | awk '/SRV.*${fqdn}\.$/ {print $1}')

          if [[ $status == "${name}.process-exporter.service.consul." ]]; then
            echo "process-exporter check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "process-exporter check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "mysql" ]]; then
        # mysql
        while true; do
          # Get the status of the mysql_check_http check
          status=$(dig @127.0.0.1 -p 8600 ${name}.percona-xtradb-cluster.service.consul SRV | awk '/SRV.*${fqdn}\.$/ {print $1}')

          if [[ $status == "${name}.percona-xtradb-cluster.service.consul." ]]; then
            echo "mysql check is passing."
            exit 0
          fi

          # If the check is not passing, wait for a short interval and try again
          echo "mysql check is not passing. Will retry in 3 seconds..."
          sleep 3
        done
      elif [[ $service == "readyz" ]] ; then
        # PMM readyz
        while true; do
          # Check for DNS :facepalm:
          dnsstatus=$(dig pmm-server.${environment_name}.local A | awk '/A.*10.*$/ {print $1}')

          if [[ $dnsstatus == "pmm-server.${environment_name}.local." ]]; then
            echo "PMMreadyz_check_http DNS check is passing."

            # Get the status of the PMMreadyz_check_http check
            status=$(dig @127.0.0.1 -p 8600 pmmreadyz.service.consul SRV | awk '/SRV.*bastion.${environment_name}.local.$/ {print $1}')
        
            if [[ $status == "pmmreadyz.service.consul." ]]; then
              echo "PMMreadyz_check_http check is passing."
              exit 0  
            fi
          else
            echo "PMMreadyz_check_http DNS check is not passing."
          fi
          # If the check is not passing, wait for a short interval and try again
          echo "PMMreadyz_check_http check is not passing. Will retry in 1 second..."
          sleep 1
        done
      fi
  - path: /root/.my.cnf
    permissions: "0600"
    content: |
      [client]
      user=root
      password="${mysql_root_password}"

  - path: /etc/my.cnf
    content: |
      [mysqld]

      # Host specific replication configuration
      #
      report_host = ${fqdn}
      report_port = 3306
      server_id   = ${index}

      # Generic
      #
      datadir=/data
      socket=/var/lib/mysql/mysql.sock
      log-error=/var/log/mysqld.log
      pid-file=/var/run/mysqld/mysqld.pid
      log_error_verbosity=3
      log_error_suppression_list='MY-013360'
      loose-validate_password.policy=0
      percona_telemetry_disable=1

      # Configure binary logs
      #
      log-bin
      binlog_format=ROW
      binlog_expire_logs_seconds=86400
      binlog_space_limit=10G

      # Configure slow logs
      #
      slow_query_log=ON
      slow_query_log_always_write_time=1
      slow_query_log_use_global_control=all
      log_slow_rate_limit=2       # Log every other slow query
      log_slow_rate_type='query'
      log_slow_admin_statements=ON
      log_slow_replica_statements=ON
      log_slow_verbosity=full
      long_query_time=0

      # InnoDB
      #
      innodb_buffer_pool_size=1G
      innodb_flush_method=O_DIRECT

      # Configure statistics
      #
      userstat=ON
      performance_schema=ON
      innodb_monitor_enable=module_index

      # wsrep
      #
      wsrep_cluster_name=pxc-80-cluster
      wsrep_node_name=${name}
      wsrep_cluster_address=gcomm://percona-xtradb-cluster-0,percona-xtradb-cluster-1,percona-xtradb-cluster-2
      wsrep_provider=/usr/lib64/galera4/libgalera_smm.so
      wsrep_applier_threads=8
      wsrep_log_conflicts
      innodb_autoinc_lock_mode=2
      pxc_strict_mode=ENFORCING
      pxc_encrypt_cluster_traffic=OFF
      wsrep_sst_method=xtrabackup-v2

      [sst]
      wsrep_debug=ON

      !includedir /etc/my.cnf.d/

  - path: /root/init-mysql.sh
    content: |
      #!/bin/bash

      # This need not be run on the replicas because replication will propagate them everywhere
      provision_users() {
        INITIAL_ROOT_PASSWORD=$(grep "root@localhost:" /var/log/mysqld.log | tail -n1 | rev | cut -d' ' -f1 | rev)
        if [ -n "$INITIAL_ROOT_PASSWORD" ]; then
          mysql --connect-expired-password -uroot -p$INITIAL_ROOT_PASSWORD -Bse "ALTER USER root@localhost IDENTIFIED BY '${mysql_root_password}'; FLUSH PRIVILEGES;"
          if [ $? -ne 0 ]; then
            echo "Error: Failed to change root password"
          fi
          echo "Password for root@localhost successfully changed"
        else
          echo "Error: Initial root password not found in /data/mysqld.log"
        fi
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'pmm-admin'@'localhost' IDENTIFIED BY '${mysql_root_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT SELECT, PROCESS, REPLICATION CLIENT, RELOAD, BACKUP_ADMIN ON *.* TO 'pmm-admin'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'consul-health-check'@'localhost' IDENTIFIED BY '${mysql_root_password}' WITH MAX_USER_CONNECTIONS 2; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT USAGE ON *.* TO 'consul-health-check'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_direct;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_proxysql;"
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE DATABASE IF NOT EXISTS sbtest_haproxy;"
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest.* TO 'pmm-admin'@'localhost'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-direct-pxc'@'%' IDENTIFIED BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_direct.* TO 'sysbench-direct-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-proxysql-pxc'@'%' IDENTIFIED BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_proxysql.* TO 'sysbench-proxysql-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER 'sysbench-haproxy-pxc'@'%' IDENTIFIED BY '${mysql_sysbench_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON sbtest_haproxy.* TO 'sysbench-haproxy-pxc'@'%'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "CREATE USER proxysql@'%' IDENTIFIED BY '${proxysql_monitor_password}'; FLUSH PRIVILEGES;";
        mysql --defaults-file=/root/.my.cnf -Bse "GRANT ALL PRIVILEGES ON *.* TO proxysql@'%'; FLUSH PRIVILEGES;";
      }

      if [[ "${name}" == "percona-xtradb-cluster-0" ]]; then
          # If we are running on the primary node (i.e., index == 0), start the PXC instance in bootstrap mode.
          systemctl start mysql@bootstrap
          provision_users
          # Check 600 times (1200sec or 20min) if PXC is ready (all 3 nodes have joined and cluster is OPERATIONAL). Switch from bootstrapping to regular operation
          for (( i=1 ; i<=600 ; i++ )); do
              if $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_evs_state';" | grep "OPERATIONAL" >/dev/null) && \
                $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_cluster_size';" | grep "3" >/dev/null)        && \
                $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_local_state_comment';" | grep "Synced" >/dev/null); then

                  # Proceed only if we have 10 successfull checks. Avoid trigger on first occurence.
                  if [[ "$${CHECK_COUNT:=1}" -ge "10" ]]; then
                      # Stop bootstrap mode
                      systemctl stop mysql@bootstrap

                      # Start primary node and break the loop
                      systemctl start mysql
                      break
                  else
                      # Increment + sleep
                      CHECK_COUNT="$(($${CHECK_COUNT:=1} + 1))"
                      sleep 1
                  fi;
              else
                  # Wait for 2sec until next retry
                  echo "retry $${i}/600"
                  sleep 2
              fi
          done
      else
          systemctl start mysql

          # Check 300 times (600sec/10min) if Primary is ready (equalize differences in deployment times - Primary can finish pre_provisoining last).
          for (( i=1 ; i<=300 ; i++ )); do
              if $(mysql --defaults-file=/root/.my.cnf -h"${fqdn}" -Bse "show global status like 'wsrep_evs_state';" | grep "OPERATIONAL" >/dev/null); then
                  # Start secondary node and brake the loop
                  systemctl start mysql
                  break
              else
                  # Wait for 2sec until next retry
                  echo "retry $${i}/300"
                  sleep 2
              fi
          done
      fi
